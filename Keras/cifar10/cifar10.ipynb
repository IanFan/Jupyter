{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 256s 2us/step\n"
     ]
    }
   ],
   "source": [
    "# 初始化\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "print(\"Initialized!\")\n",
    "\n",
    "# 定義變量\n",
    "batch_size = 32\n",
    "nb_classes = 10\n",
    "nb_epoch = 50\n",
    "img_rows, img_cols = 32, 32\n",
    "nb_filters = [32, 32, 64, 64]\n",
    "pool_size = (2, 2)\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "#\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test  = X_test.astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train\n",
    "y_test = y_test\n",
    "\n",
    "\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            rotation_range=0,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=False)\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0216 14:01:26.691919 4580275648 deprecation.py:506] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(nb_filters[0], kernel_size, padding='same',input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(nb_filters[1], kernel_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(nb_filters[2], kernel_size, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(nb_filters[3], kernel_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=adam,\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1562/1562 [==============================] - 119s 76ms/step - loss: 1.8150 - acc: 0.3283 - val_loss: 1.4913 - val_acc: 0.4604\n",
      "Epoch 2/50\n",
      "1562/1562 [==============================] - 124s 80ms/step - loss: 1.5317 - acc: 0.4388 - val_loss: 1.3371 - val_acc: 0.5107\n",
      "Epoch 3/50\n",
      "1562/1562 [==============================] - 118s 76ms/step - loss: 1.4270 - acc: 0.4831 - val_loss: 1.2600 - val_acc: 0.5439\n",
      "Epoch 4/50\n",
      "1562/1562 [==============================] - 156s 100ms/step - loss: 1.3476 - acc: 0.5151 - val_loss: 1.2408 - val_acc: 0.5603\n",
      "Epoch 5/50\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 1.2826 - acc: 0.5386 - val_loss: 1.1120 - val_acc: 0.6051\n",
      "Epoch 6/50\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 1.2275 - acc: 0.5622 - val_loss: 1.0318 - val_acc: 0.6347\n",
      "Epoch 7/50\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 1.1712 - acc: 0.5837 - val_loss: 1.1084 - val_acc: 0.6085\n",
      "Epoch 8/50\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 1.1241 - acc: 0.6026 - val_loss: 0.9795 - val_acc: 0.6556\n",
      "Epoch 9/50\n",
      "1562/1562 [==============================] - 166s 107ms/step - loss: 1.0960 - acc: 0.6125 - val_loss: 0.9779 - val_acc: 0.6594\n",
      "Epoch 10/50\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 1.0543 - acc: 0.6274 - val_loss: 0.9255 - val_acc: 0.6727\n",
      "Epoch 11/50\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 1.0310 - acc: 0.6384 - val_loss: 0.9141 - val_acc: 0.6772\n",
      "Epoch 12/50\n",
      "1562/1562 [==============================] - 174s 111ms/step - loss: 0.9986 - acc: 0.6478 - val_loss: 0.8548 - val_acc: 0.7013\n",
      "Epoch 13/50\n",
      "1562/1562 [==============================] - 177s 113ms/step - loss: 0.9697 - acc: 0.6590 - val_loss: 0.8460 - val_acc: 0.7062\n",
      "Epoch 14/50\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 0.9495 - acc: 0.6679 - val_loss: 0.8026 - val_acc: 0.7164\n",
      "Epoch 15/50\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 0.9324 - acc: 0.6732 - val_loss: 0.8141 - val_acc: 0.7170\n",
      "Epoch 16/50\n",
      "1562/1562 [==============================] - 165s 106ms/step - loss: 0.9090 - acc: 0.6804 - val_loss: 0.8023 - val_acc: 0.7209\n",
      "Epoch 17/50\n",
      "1562/1562 [==============================] - 115s 74ms/step - loss: 0.8917 - acc: 0.6884 - val_loss: 0.9100 - val_acc: 0.6841\n",
      "Epoch 18/50\n",
      "1562/1562 [==============================] - 114s 73ms/step - loss: 0.8756 - acc: 0.6921 - val_loss: 0.7408 - val_acc: 0.7448\n",
      "Epoch 19/50\n",
      "1562/1562 [==============================] - 124s 79ms/step - loss: 0.8612 - acc: 0.6995 - val_loss: 0.7536 - val_acc: 0.7402\n",
      "Epoch 20/50\n",
      "1562/1562 [==============================] - 126s 80ms/step - loss: 0.8485 - acc: 0.7040 - val_loss: 0.7283 - val_acc: 0.7468\n",
      "Epoch 21/50\n",
      "1562/1562 [==============================] - 119s 76ms/step - loss: 0.8363 - acc: 0.7089 - val_loss: 0.7029 - val_acc: 0.7594\n",
      "Epoch 22/50\n",
      "1562/1562 [==============================] - 115s 74ms/step - loss: 0.8176 - acc: 0.7142 - val_loss: 0.7488 - val_acc: 0.7394\n",
      "Epoch 23/50\n",
      "1562/1562 [==============================] - 118s 76ms/step - loss: 0.8112 - acc: 0.7185 - val_loss: 0.6907 - val_acc: 0.7616\n",
      "Epoch 24/50\n",
      "1562/1562 [==============================] - 117s 75ms/step - loss: 0.8012 - acc: 0.7207 - val_loss: 0.6691 - val_acc: 0.7648\n",
      "Epoch 25/50\n",
      "1562/1562 [==============================] - 115s 74ms/step - loss: 0.7947 - acc: 0.7209 - val_loss: 0.7055 - val_acc: 0.7547\n",
      "Epoch 26/50\n",
      "1562/1562 [==============================] - 115s 73ms/step - loss: 0.7810 - acc: 0.7279 - val_loss: 0.6909 - val_acc: 0.7619\n",
      "Epoch 27/50\n",
      "1562/1562 [==============================] - 115s 73ms/step - loss: 0.7718 - acc: 0.7317 - val_loss: 0.6746 - val_acc: 0.7674\n",
      "Epoch 28/50\n",
      "1562/1562 [==============================] - 116s 74ms/step - loss: 0.7657 - acc: 0.7312 - val_loss: 0.6732 - val_acc: 0.7675\n",
      "Epoch 29/50\n",
      "1562/1562 [==============================] - 115s 74ms/step - loss: 0.7540 - acc: 0.7374 - val_loss: 0.6617 - val_acc: 0.7721\n",
      "Epoch 30/50\n",
      "1562/1562 [==============================] - 116s 74ms/step - loss: 0.7493 - acc: 0.7391 - val_loss: 0.6435 - val_acc: 0.7765\n",
      "Epoch 31/50\n",
      "1562/1562 [==============================] - 120s 77ms/step - loss: 0.7379 - acc: 0.7444 - val_loss: 0.6269 - val_acc: 0.7796\n",
      "Epoch 32/50\n",
      "1562/1562 [==============================] - 118s 75ms/step - loss: 0.7291 - acc: 0.7450 - val_loss: 0.6426 - val_acc: 0.7773\n",
      "Epoch 33/50\n",
      "1562/1562 [==============================] - 116s 74ms/step - loss: 0.7203 - acc: 0.7471 - val_loss: 0.6261 - val_acc: 0.7814\n",
      "Epoch 34/50\n",
      "1562/1562 [==============================] - 115s 74ms/step - loss: 0.7206 - acc: 0.7491 - val_loss: 0.6370 - val_acc: 0.7773\n",
      "Epoch 35/50\n",
      "1562/1562 [==============================] - 117s 75ms/step - loss: 0.7052 - acc: 0.7561 - val_loss: 0.6309 - val_acc: 0.7800\n",
      "Epoch 36/50\n",
      "1562/1562 [==============================] - 116s 74ms/step - loss: 0.7059 - acc: 0.7534 - val_loss: 0.6375 - val_acc: 0.7813\n",
      "Epoch 37/50\n",
      "1562/1562 [==============================] - 116s 75ms/step - loss: 0.6906 - acc: 0.7575 - val_loss: 0.6267 - val_acc: 0.7836\n",
      "Epoch 38/50\n",
      "1562/1562 [==============================] - 116s 74ms/step - loss: 0.6909 - acc: 0.7593 - val_loss: 0.6126 - val_acc: 0.7835\n",
      "Epoch 39/50\n",
      "1562/1562 [==============================] - 115s 74ms/step - loss: 0.6825 - acc: 0.7624 - val_loss: 0.6207 - val_acc: 0.7860\n",
      "Epoch 40/50\n",
      "1562/1562 [==============================] - 116s 74ms/step - loss: 0.6812 - acc: 0.7611 - val_loss: 0.6182 - val_acc: 0.7869\n",
      "Epoch 41/50\n",
      "1562/1562 [==============================] - 119s 76ms/step - loss: 0.6726 - acc: 0.7626 - val_loss: 0.5895 - val_acc: 0.7978\n",
      "Epoch 42/50\n",
      "1562/1562 [==============================] - 116s 74ms/step - loss: 0.6712 - acc: 0.7674 - val_loss: 0.5911 - val_acc: 0.7933\n",
      "Epoch 43/50\n",
      "1562/1562 [==============================] - 115s 74ms/step - loss: 0.6617 - acc: 0.7707 - val_loss: 0.5862 - val_acc: 0.7954\n",
      "Epoch 44/50\n",
      "1562/1562 [==============================] - 116s 74ms/step - loss: 0.6596 - acc: 0.7725 - val_loss: 0.5743 - val_acc: 0.7997\n",
      "Epoch 45/50\n",
      "1562/1562 [==============================] - 117s 75ms/step - loss: 0.6543 - acc: 0.7712 - val_loss: 0.5612 - val_acc: 0.8043\n",
      "Epoch 46/50\n",
      "1562/1562 [==============================] - 115s 74ms/step - loss: 0.6489 - acc: 0.7730 - val_loss: 0.5945 - val_acc: 0.7935\n",
      "Epoch 47/50\n",
      "1562/1562 [==============================] - 119s 76ms/step - loss: 0.6459 - acc: 0.7769 - val_loss: 0.5737 - val_acc: 0.8015\n",
      "Epoch 48/50\n",
      "1562/1562 [==============================] - 116s 74ms/step - loss: 0.6388 - acc: 0.7777 - val_loss: 0.5868 - val_acc: 0.8004\n",
      "Epoch 49/50\n",
      "1562/1562 [==============================] - 116s 74ms/step - loss: 0.6365 - acc: 0.7776 - val_loss: 0.5490 - val_acc: 0.8101\n",
      "Epoch 50/50\n",
      "1562/1562 [==============================] - 116s 75ms/step - loss: 0.6293 - acc: 0.7809 - val_loss: 0.5784 - val_acc: 0.7994\n",
      "Test score: 0.5784492125988007\n",
      "Accuracy: 79.94%\n",
      "Compiled!\n"
     ]
    }
   ],
   "source": [
    "#訓練模型\n",
    "best_model = ModelCheckpoint(\"cifar10_best.h5\", monitor='val_loss', verbose=0, save_best_only=True)\n",
    "tb = TensorBoard(log_dir=\"./logs\")\n",
    "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                        epochs=nb_epoch, verbose=1,\n",
    "                        validation_data=(X_test, Y_test), callbacks=[best_model,tb])\n",
    "\n",
    "\n",
    "# 模型評分\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "# 輸出結果\n",
    "print('Test score:', score[0])\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))                   \n",
    "print(\"Compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
