{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0224 18:59:27.708060 4594804160 deprecation.py:323] From <ipython-input-1-8bf8ae5a5303>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0224 18:59:27.709332 4594804160 deprecation.py:323] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0224 18:59:27.710293 4594804160 deprecation.py:323] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0224 18:59:27.893651 4594804160 deprecation.py:323] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0224 18:59:27.896093 4594804160 deprecation.py:323] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0224 18:59:27.939557 4594804160 deprecation.py:323] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, MaxPooling2D, Flatten\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import np_utils, plot_model\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython.display import Image\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 28*28)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 784)               101136    \n",
      "=================================================================\n",
      "Total params: 114,064\n",
      "Trainable params: 114,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,609\n",
      "Trainable params: 0\n",
      "Non-trainable params: 100,609\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_1 (Model)              (None, 1)                 100609    \n",
      "=================================================================\n",
      "Total params: 201,218\n",
      "Trainable params: 100,609\n",
      "Non-trainable params: 100,609\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_2 (Model)              (None, 784)               114064    \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 1)                 100609    \n",
      "=================================================================\n",
      "Total params: 214,673\n",
      "Trainable params: 114,064\n",
      "Non-trainable params: 100,609\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "x_dim = x_train.shape[1] # 784\n",
    "z_dim = 100\n",
    "h_dim = 128\n",
    "batch_size = 128\n",
    "\n",
    "x = Input(shape=(x_dim,))\n",
    "D_h = Dense(h_dim)(x)\n",
    "D_h = LeakyReLU(0.2)(D_h)\n",
    "D_o = Dense(1, activation='sigmoid')(D_h)\n",
    "D = Model(x, D_o)\n",
    "\n",
    "z = Input(shape=(z_dim,))\n",
    "G_h = Dense(h_dim)(z)\n",
    "G_h = LeakyReLU(0.2)(G_h)\n",
    "G_o = Dense(x_dim, activation='sigmoid')(G_h)\n",
    "G = Model(z, G_o)\n",
    "\n",
    "# del D\n",
    "# D = load_model('D.h5')\n",
    "# del G\n",
    "# G = load_model('G.h5')\n",
    "\n",
    "DM = Sequential()\n",
    "D.trainable = True\n",
    "DM.add(D)\n",
    "DM.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "AM = Sequential()\n",
    "AM.add(G)\n",
    "D.trainable = False\n",
    "AM.add(D)\n",
    "AM.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "G.summary()\n",
    "D.summary()\n",
    "DM.summary()\n",
    "AM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0\n",
      "[D loss:0.4933, acc:0.5938] [G loss:0.6846, acc:0.4922]\n",
      "\n",
      "iter:1000\n",
      "[D loss:0.1039, acc:0.9844] [G loss:3.351, acc:0.007812]\n",
      "\n",
      "iter:2000\n",
      "[D loss:0.1122, acc:0.9766] [G loss:4.292, acc:0.05469]\n",
      "\n",
      "iter:3000\n",
      "[D loss:0.3262, acc:0.875] [G loss:4.648, acc:0.007812]\n",
      "\n",
      "iter:4000\n",
      "[D loss:0.9009, acc:0.582] [G loss:1.804, acc:0.2734]\n",
      "\n",
      "iter:5000\n",
      "[D loss:0.4827, acc:0.7656] [G loss:3.641, acc:0.1328]\n",
      "\n",
      "iter:6000\n",
      "[D loss:1.117, acc:0.4648] [G loss:1.442, acc:0.2656]\n",
      "\n",
      "iter:7000\n",
      "[D loss:0.8408, acc:0.6016] [G loss:2.164, acc:0.1562]\n",
      "\n",
      "iter:8000\n",
      "[D loss:0.6239, acc:0.7109] [G loss:2.359, acc:0.1016]\n",
      "\n",
      "iter:9000\n",
      "[D loss:0.7168, acc:0.6484] [G loss:1.765, acc:0.25]\n",
      "\n",
      "iter:10000\n",
      "[D loss:0.7662, acc:0.5781] [G loss:1.502, acc:0.25]\n",
      "\n",
      "iter:11000\n",
      "[D loss:0.9894, acc:0.5273] [G loss:1.369, acc:0.3281]\n",
      "\n",
      "iter:12000\n",
      "[D loss:0.7875, acc:0.6289] [G loss:1.63, acc:0.3359]\n",
      "\n",
      "iter:13000\n",
      "[D loss:1.07, acc:0.5039] [G loss:1.516, acc:0.4453]\n",
      "\n",
      "iter:14000\n",
      "[D loss:0.6681, acc:0.6641] [G loss:1.474, acc:0.2109]\n",
      "\n",
      "iter:15000\n",
      "[D loss:0.5078, acc:0.7656] [G loss:1.615, acc:0.1016]\n",
      "\n",
      "iter:16000\n",
      "[D loss:0.6164, acc:0.7266] [G loss:1.798, acc:0.2344]\n",
      "\n",
      "iter:17000\n",
      "[D loss:0.6281, acc:0.6797] [G loss:1.378, acc:0.2344]\n",
      "\n",
      "iter:18000\n",
      "[D loss:0.6042, acc:0.7305] [G loss:1.865, acc:0.1953]\n",
      "\n",
      "iter:19000\n",
      "[D loss:1.107, acc:0.5352] [G loss:1.235, acc:0.4062]\n",
      "\n",
      "iter:20000\n",
      "[D loss:0.7494, acc:0.6641] [G loss:1.867, acc:0.2266]\n",
      "\n",
      "iter:21000\n",
      "[D loss:0.5584, acc:0.7305] [G loss:2.073, acc:0.05469]\n",
      "\n",
      "iter:22000\n",
      "[D loss:0.6712, acc:0.6484] [G loss:1.508, acc:0.1172]\n",
      "\n",
      "iter:23000\n",
      "[D loss:0.5829, acc:0.7305] [G loss:2.648, acc:0.1016]\n",
      "\n",
      "iter:24000\n",
      "[D loss:0.6796, acc:0.6875] [G loss:1.281, acc:0.3047]\n",
      "\n",
      "iter:25000\n",
      "[D loss:0.7244, acc:0.6523] [G loss:2.387, acc:0.1328]\n",
      "\n",
      "iter:26000\n",
      "[D loss:0.8234, acc:0.5742] [G loss:0.8591, acc:0.5078]\n",
      "\n",
      "iter:27000\n",
      "[D loss:0.9274, acc:0.5312] [G loss:0.9088, acc:0.5469]\n",
      "\n",
      "iter:28000\n",
      "[D loss:0.6923, acc:0.6406] [G loss:1.596, acc:0.1719]\n",
      "\n",
      "iter:29000\n",
      "[D loss:0.6125, acc:0.7461] [G loss:1.652, acc:0.2422]\n",
      "\n",
      "iter:30000\n",
      "[D loss:0.6897, acc:0.6719] [G loss:1.718, acc:0.2109]\n",
      "\n",
      "iter:31000\n",
      "[D loss:0.7149, acc:0.6836] [G loss:1.769, acc:0.2656]\n",
      "\n",
      "iter:32000\n",
      "[D loss:0.7496, acc:0.6211] [G loss:1.364, acc:0.2969]\n",
      "\n",
      "iter:33000\n",
      "[D loss:0.7114, acc:0.6562] [G loss:1.472, acc:0.2969]\n",
      "\n",
      "iter:34000\n",
      "[D loss:0.6012, acc:0.6875] [G loss:1.345, acc:0.2969]\n",
      "\n",
      "iter:35000\n",
      "[D loss:0.7025, acc:0.6172] [G loss:1.62, acc:0.2422]\n",
      "\n",
      "iter:36000\n",
      "[D loss:0.7019, acc:0.6484] [G loss:1.325, acc:0.3047]\n",
      "\n",
      "iter:37000\n",
      "[D loss:0.7326, acc:0.6484] [G loss:1.291, acc:0.3203]\n",
      "\n",
      "iter:38000\n",
      "[D loss:0.5825, acc:0.7148] [G loss:1.652, acc:0.2656]\n",
      "\n",
      "iter:39000\n",
      "[D loss:0.6224, acc:0.6914] [G loss:1.534, acc:0.1875]\n",
      "\n",
      "iter:40000\n",
      "[D loss:0.4888, acc:0.7656] [G loss:1.465, acc:0.1719]\n",
      "\n",
      "iter:41000\n",
      "[D loss:0.6201, acc:0.6719] [G loss:1.788, acc:0.1641]\n",
      "\n",
      "iter:42000\n",
      "[D loss:0.7389, acc:0.6367] [G loss:1.626, acc:0.2188]\n",
      "\n",
      "iter:43000\n",
      "[D loss:0.8197, acc:0.582] [G loss:1.352, acc:0.2812]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5f6923eb94f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mG_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1439\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "    return fig\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m,n])\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i=0\n",
    "for it in range(1000000):\n",
    "    r = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    x_real_batch = x_train[r]\n",
    "    \n",
    "    samples = sample_Z(batch_size, z_dim)\n",
    "    x_fake_batch = G.predict(samples)\n",
    "    \n",
    "    real = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    x_D = np.concatenate([x_real_batch, x_fake_batch])\n",
    "    y_D = np.concatenate([real, fake])\n",
    "    \n",
    "    D_loss, D_acc = DM.train_on_batch(x_D, y_D)\n",
    "    \n",
    "    samples = sample_Z(batch_size, z_dim)\n",
    "    \n",
    "    G_loss, G_acc = AM.train_on_batch(samples, real)\n",
    "    \n",
    "    if it%1000==0:\n",
    "        D.save('D.h5')\n",
    "        G.save('G.h5')\n",
    "        \n",
    "        samples = G.predict(sample_Z(16, z_dim))\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i+=1\n",
    "        plt.close(fig)\n",
    "        \n",
    "        print('iter:{}'.format(it))\n",
    "        print('[D loss:{:.4}, acc:{:.4}] [G loss:{:.4}, acc:{:.4}]'.format(D_loss, D_acc, G_loss, G_acc))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
