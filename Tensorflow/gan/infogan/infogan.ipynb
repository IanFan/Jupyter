{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0227 17:22:46.231209 4556686784 deprecation.py:323] From <ipython-input-1-3c4f46c1a93b>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0227 17:22:46.232139 4556686784 deprecation.py:323] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0227 17:22:46.233330 4556686784 deprecation.py:323] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0227 17:22:46.414200 4556686784 deprecation.py:323] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0227 17:22:46.416793 4556686784 deprecation.py:323] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0227 17:22:46.462126 4556686784 deprecation.py:323] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../../MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True) \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['out/', 'model/']:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0227 17:22:46.889415 4556686784 deprecation.py:506] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py:187: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "mb_size = 32\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "Z_dim = 16\n",
    "c_dim_cat = 10\n",
    "c_dim_con = 2\n",
    "eps = 1e-8\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1./tf.sqrt(in_dim/2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([X_dim, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "def discriminator(X):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(X, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, Z_dim])\n",
    "c_cat = tf.placeholder(tf.float32, shape=[None, c_dim_cat])\n",
    "c_con = tf.placeholder(tf.float32, shape=[None, c_dim_con])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([Z_dim + c_dim_cat + c_dim_con, 256]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[256]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([256, X_dim]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "def generator(z, c_cat, c_con):\n",
    "    inputs = tf.concat(axis=1, values=[z, c_cat, c_con])\n",
    "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    return G_prob\n",
    "\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim, 128]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "Q_W2_cat = tf.Variable(xavier_init([128, c_dim_cat]))\n",
    "Q_b2_cat = tf.Variable(tf.zeros(shape=[c_dim_cat]))\n",
    "\n",
    "Q_W2_con = tf.Variable(xavier_init([128, c_dim_con]))\n",
    "Q_b2_con = tf.Variable(tf.zeros(shape=[c_dim_con]))\n",
    "\n",
    "theta_Q = [Q_W1, Q_W2_cat, Q_W2_con, Q_b1, Q_b2_cat, Q_b2_con]\n",
    "\n",
    "def Q(x):\n",
    "    Q_h1 = tf.nn.relu(tf.matmul(x, Q_W1) + Q_b1)\n",
    "    Q_cat_prob = tf.nn.softmax(tf.matmul(Q_h1, Q_W2_cat) + Q_b2_cat)\n",
    "    Q_con = tf.matmul(Q_h1, Q_W2_con) + Q_b2_con\n",
    "    return Q_cat_prob, Q_con\n",
    "\n",
    "def categorical_crossentropy_with_prob(prob, y):\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y*tf.log(prob + eps), axis=1))\n",
    "\n",
    "def mean_squared_error(prob, y):\n",
    "    return tf.reduce_mean(tf.square(y - prob))\n",
    "#     return tf.losses.mean_squared_error(y, prob)\n",
    "\n",
    "def sample_Z(m,n):\n",
    "    return np.random.uniform(-1., 1., size=[m,n])\n",
    "\n",
    "def sample_c_cat(m, n):\n",
    "    return np.random.multinomial(1, n*[0.1], size=m)\n",
    "\n",
    "def sample_c_con(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m,n])\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(8, 8)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "    return fig\n",
    "\n",
    "G_sample = generator(Z, c_cat, c_con)\n",
    "D_real = discriminator(X)\n",
    "D_fake = discriminator(G_sample)\n",
    "Q_c_cat_given_x, Q_c_con_given_x = Q(G_sample)\n",
    "\n",
    "D_loss = -tf.reduce_mean(tf.log(D_real + eps) + tf.log(1. - D_fake + eps))\n",
    "G_loss = -tf.reduce_mean(tf.log(D_fake + eps))\n",
    "\n",
    "Q_loss_cat = categorical_crossentropy_with_prob(Q_c_cat_given_x, c_cat)\n",
    "Q_loss_con = mean_squared_error(Q_c_con_given_x, c_con)\n",
    "Q_loss = Q_loss_cat + Q_loss_con\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "Q_solver = tf.train.AdamOptimizer().minimize(Q_loss, var_list=theta_G + theta_Q)\n",
    "\n",
    "D_real_predict = tf.cast(tf.greater_equal(D_real, 0.5), tf.float32)\n",
    "D_real_correct = tf.cast(tf.equal(D_real_predict, np.ones_like([mb_size, 1])), tf.float32)\n",
    "D_real_accuracy = tf.reduce_mean(D_real_correct)\n",
    "\n",
    "D_fake_predict = tf.cast(tf.greater_equal(D_fake, 0.5), tf.float32)\n",
    "D_fake_correct = tf.cast(tf.equal(D_fake_predict, np.zeros_like([mb_size, 1])), tf.float32)\n",
    "D_fake_accuracy = tf.reduce_mean(D_fake_correct)\n",
    "\n",
    "D_accuracy = 0.5 * (D_real_accuracy + D_fake_accuracy)\n",
    "\n",
    "G_predict = tf.cast(tf.greater_equal(D_fake, 0.5), tf.float32)\n",
    "G_correct = tf.cast(tf.equal(G_predict, np.ones_like([mb_size, 1])), tf.float32)\n",
    "G_accuracy = tf.reduce_mean(G_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0227 17:22:47.334863 4556686784 deprecation.py:323] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"model/save_net.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0\n",
      "[D loss:0.4742 acc:0.9219] [G loss:3.494 acc:0.0] [Q loss:0.002363]\n",
      "\n",
      "iter:1000\n",
      "[D loss:0.2754 acc:0.9844] [G loss:3.044 acc:0.0] [Q loss:0.002527]\n",
      "\n",
      "iter:2000\n",
      "[D loss:0.5205 acc:0.9062] [G loss:2.105 acc:0.09375] [Q loss:0.00296]\n",
      "\n",
      "iter:3000\n",
      "[D loss:1.042 acc:0.7969] [G loss:2.212 acc:0.0625] [Q loss:0.003496]\n",
      "\n",
      "iter:4000\n",
      "[D loss:0.438 acc:0.9375] [G loss:2.457 acc:0.0625] [Q loss:0.003589]\n",
      "\n",
      "iter:5000\n",
      "[D loss:0.8396 acc:0.8281] [G loss:2.155 acc:0.0625] [Q loss:0.001597]\n",
      "\n",
      "iter:6000\n",
      "[D loss:0.3867 acc:0.9531] [G loss:2.221 acc:0.0] [Q loss:0.00193]\n",
      "\n",
      "iter:7000\n",
      "[D loss:0.5587 acc:0.9219] [G loss:2.922 acc:0.0] [Q loss:0.00225]\n",
      "\n",
      "iter:8000\n",
      "[D loss:0.4622 acc:0.9062] [G loss:2.671 acc:0.0] [Q loss:0.002473]\n",
      "\n",
      "iter:9000\n",
      "[D loss:0.4512 acc:0.9375] [G loss:2.842 acc:0.03125] [Q loss:0.004488]\n",
      "\n",
      "iter:10000\n",
      "[D loss:0.1601 acc:0.9844] [G loss:2.967 acc:0.0] [Q loss:0.002819]\n",
      "\n",
      "iter:11000\n",
      "[D loss:0.588 acc:0.9219] [G loss:1.987 acc:0.09375] [Q loss:0.002176]\n",
      "\n",
      "iter:12000\n",
      "[D loss:0.7542 acc:0.8906] [G loss:2.302 acc:0.03125] [Q loss:0.002975]\n",
      "\n",
      "iter:13000\n",
      "[D loss:0.2996 acc:0.9688] [G loss:2.932 acc:0.0] [Q loss:0.003045]\n",
      "\n",
      "iter:14000\n",
      "[D loss:0.5542 acc:0.8906] [G loss:3.316 acc:0.0] [Q loss:0.002375]\n",
      "\n",
      "iter:15000\n",
      "[D loss:0.5746 acc:0.9531] [G loss:2.311 acc:0.0] [Q loss:0.001664]\n",
      "\n",
      "iter:16000\n",
      "[D loss:0.604 acc:0.8906] [G loss:2.383 acc:0.09375] [Q loss:0.00438]\n",
      "\n",
      "iter:17000\n",
      "[D loss:0.6121 acc:0.9062] [G loss:2.554 acc:0.0625] [Q loss:0.003116]\n",
      "\n",
      "iter:18000\n",
      "[D loss:0.4087 acc:0.9375] [G loss:2.363 acc:0.0] [Q loss:0.003785]\n",
      "\n",
      "iter:19000\n",
      "[D loss:0.5187 acc:0.9219] [G loss:3.002 acc:0.0] [Q loss:0.003672]\n",
      "\n",
      "iter:20000\n",
      "[D loss:0.7894 acc:0.8906] [G loss:3.251 acc:0.03125] [Q loss:0.003002]\n",
      "\n",
      "iter:21000\n",
      "[D loss:0.3338 acc:0.9375] [G loss:2.307 acc:0.03125] [Q loss:0.003806]\n",
      "\n",
      "iter:22000\n",
      "[D loss:0.5525 acc:0.9219] [G loss:2.974 acc:0.03125] [Q loss:0.00287]\n",
      "\n",
      "iter:23000\n",
      "[D loss:0.3509 acc:0.9375] [G loss:2.484 acc:0.03125] [Q loss:0.002726]\n",
      "\n",
      "iter:24000\n",
      "[D loss:0.4475 acc:0.9688] [G loss:2.669 acc:0.0] [Q loss:0.002227]\n",
      "\n",
      "iter:25000\n",
      "[D loss:0.4072 acc:0.9062] [G loss:2.878 acc:0.03125] [Q loss:0.002504]\n",
      "\n",
      "iter:26000\n",
      "[D loss:0.6578 acc:0.8594] [G loss:2.995 acc:0.0625] [Q loss:0.002537]\n",
      "\n",
      "iter:27000\n",
      "[D loss:0.3042 acc:0.9844] [G loss:2.564 acc:0.03125] [Q loss:0.003106]\n",
      "\n",
      "iter:28000\n",
      "[D loss:0.411 acc:0.9375] [G loss:2.69 acc:0.03125] [Q loss:0.002009]\n",
      "\n",
      "iter:29000\n",
      "[D loss:0.4328 acc:0.9219] [G loss:2.264 acc:0.03125] [Q loss:0.00234]\n",
      "\n",
      "iter:30000\n",
      "[D loss:0.6012 acc:0.8906] [G loss:2.709 acc:0.0625] [Q loss:0.002423]\n",
      "\n",
      "iter:31000\n",
      "[D loss:0.4855 acc:0.9375] [G loss:2.836 acc:0.03125] [Q loss:0.004211]\n",
      "\n",
      "iter:32000\n",
      "[D loss:0.8879 acc:0.9062] [G loss:2.325 acc:0.0] [Q loss:0.002342]\n",
      "\n",
      "iter:33000\n",
      "[D loss:0.7677 acc:0.8594] [G loss:2.412 acc:0.0625] [Q loss:0.002074]\n",
      "\n",
      "iter:34000\n",
      "[D loss:0.4125 acc:0.9219] [G loss:2.823 acc:0.0625] [Q loss:0.002296]\n",
      "\n",
      "iter:35000\n",
      "[D loss:0.1906 acc:0.9844] [G loss:2.85 acc:0.0] [Q loss:0.002163]\n",
      "\n",
      "iter:36000\n",
      "[D loss:0.2798 acc:0.9688] [G loss:2.724 acc:0.0] [Q loss:0.002312]\n",
      "\n",
      "iter:37000\n",
      "[D loss:0.6976 acc:0.9062] [G loss:2.431 acc:0.0] [Q loss:0.002733]\n",
      "\n",
      "iter:38000\n",
      "[D loss:0.4155 acc:0.9062] [G loss:2.597 acc:0.03125] [Q loss:0.002913]\n",
      "\n",
      "iter:39000\n",
      "[D loss:0.3268 acc:0.9844] [G loss:3.145 acc:0.0] [Q loss:0.003286]\n",
      "\n",
      "iter:40000\n",
      "[D loss:0.2324 acc:0.9688] [G loss:2.756 acc:0.03125] [Q loss:0.001796]\n",
      "\n",
      "iter:41000\n",
      "[D loss:0.5188 acc:0.9062] [G loss:2.809 acc:0.03125] [Q loss:0.002554]\n",
      "\n",
      "iter:42000\n",
      "[D loss:0.2678 acc:0.9688] [G loss:2.942 acc:0.0] [Q loss:0.002581]\n",
      "\n",
      "iter:43000\n",
      "[D loss:0.4506 acc:0.9062] [G loss:2.551 acc:0.0625] [Q loss:0.003188]\n",
      "\n",
      "iter:44000\n",
      "[D loss:0.2375 acc:0.9688] [G loss:2.701 acc:0.0] [Q loss:0.003624]\n",
      "\n",
      "iter:45000\n",
      "[D loss:0.2398 acc:0.9844] [G loss:2.659 acc:0.0] [Q loss:0.001637]\n",
      "\n",
      "iter:46000\n",
      "[D loss:0.3491 acc:0.9531] [G loss:2.723 acc:0.0] [Q loss:0.004011]\n",
      "\n",
      "iter:47000\n",
      "[D loss:0.431 acc:0.9375] [G loss:3.322 acc:0.03125] [Q loss:0.004546]\n",
      "\n",
      "iter:48000\n",
      "[D loss:0.3609 acc:0.9688] [G loss:2.659 acc:0.0] [Q loss:0.003087]\n",
      "\n",
      "iter:49000\n",
      "[D loss:0.2402 acc:0.9688] [G loss:2.949 acc:0.03125] [Q loss:0.002508]\n",
      "\n",
      "iter:50000\n",
      "[D loss:0.5602 acc:0.9062] [G loss:2.733 acc:0.03125] [Q loss:0.003741]\n",
      "\n",
      "iter:51000\n",
      "[D loss:0.4554 acc:0.9219] [G loss:3.581 acc:0.0] [Q loss:0.00356]\n",
      "\n",
      "iter:52000\n",
      "[D loss:0.433 acc:0.9375] [G loss:3.522 acc:0.0] [Q loss:0.002343]\n",
      "\n",
      "iter:53000\n",
      "[D loss:0.3021 acc:0.9531] [G loss:2.621 acc:0.0] [Q loss:0.003559]\n",
      "\n",
      "iter:54000\n",
      "[D loss:0.2711 acc:0.9688] [G loss:3.247 acc:0.03125] [Q loss:0.003679]\n",
      "\n",
      "iter:55000\n",
      "[D loss:0.474 acc:0.8906] [G loss:2.324 acc:0.0625] [Q loss:0.002129]\n",
      "\n",
      "iter:56000\n",
      "[D loss:0.4011 acc:0.9375] [G loss:2.851 acc:0.03125] [Q loss:0.003117]\n",
      "\n",
      "iter:57000\n",
      "[D loss:0.3627 acc:0.9219] [G loss:3.134 acc:0.0] [Q loss:0.002464]\n",
      "\n",
      "iter:58000\n",
      "[D loss:0.3104 acc:0.9531] [G loss:2.44 acc:0.03125] [Q loss:0.003795]\n",
      "\n",
      "iter:59000\n",
      "[D loss:0.4665 acc:0.9219] [G loss:2.845 acc:0.0] [Q loss:0.003815]\n",
      "\n",
      "iter:60000\n",
      "[D loss:0.5997 acc:0.9219] [G loss:3.23 acc:0.03125] [Q loss:0.002297]\n",
      "\n",
      "iter:61000\n",
      "[D loss:0.07862 acc:1.0] [G loss:4.256 acc:0.0] [Q loss:0.003473]\n",
      "\n",
      "iter:62000\n",
      "[D loss:0.2872 acc:0.9531] [G loss:3.436 acc:0.0] [Q loss:0.003889]\n",
      "\n",
      "iter:63000\n",
      "[D loss:0.418 acc:0.9531] [G loss:2.829 acc:0.0] [Q loss:0.004735]\n",
      "\n",
      "iter:64000\n",
      "[D loss:0.3247 acc:0.9219] [G loss:3.196 acc:0.0] [Q loss:0.004949]\n",
      "\n",
      "iter:65000\n",
      "[D loss:0.3486 acc:0.9375] [G loss:3.404 acc:0.0] [Q loss:0.005199]\n",
      "\n",
      "iter:66000\n",
      "[D loss:0.2826 acc:0.9531] [G loss:3.118 acc:0.03125] [Q loss:0.003774]\n",
      "\n",
      "iter:67000\n",
      "[D loss:0.1594 acc:0.9844] [G loss:3.466 acc:0.0] [Q loss:0.004902]\n",
      "\n",
      "iter:68000\n",
      "[D loss:0.3687 acc:0.9375] [G loss:3.089 acc:0.03125] [Q loss:0.01056]\n",
      "\n",
      "iter:69000\n",
      "[D loss:0.4424 acc:0.9219] [G loss:3.584 acc:0.0] [Q loss:0.004098]\n",
      "\n",
      "iter:70000\n",
      "[D loss:0.3627 acc:0.9219] [G loss:2.611 acc:0.0] [Q loss:0.002457]\n",
      "\n",
      "iter:71000\n",
      "[D loss:0.3942 acc:0.9531] [G loss:2.312 acc:0.0] [Q loss:0.002747]\n",
      "\n",
      "iter:72000\n",
      "[D loss:0.3313 acc:0.9531] [G loss:3.287 acc:0.0] [Q loss:0.00366]\n",
      "\n",
      "iter:73000\n",
      "[D loss:0.263 acc:0.9531] [G loss:3.616 acc:0.03125] [Q loss:0.005249]\n",
      "\n",
      "iter:74000\n",
      "[D loss:0.2523 acc:0.9688] [G loss:3.554 acc:0.0] [Q loss:0.002605]\n",
      "\n",
      "iter:75000\n",
      "[D loss:0.3301 acc:0.9375] [G loss:3.225 acc:0.0625] [Q loss:0.00438]\n",
      "\n",
      "iter:76000\n",
      "[D loss:0.3309 acc:0.9375] [G loss:2.781 acc:0.03125] [Q loss:0.003323]\n",
      "\n",
      "iter:77000\n",
      "[D loss:0.3651 acc:0.8906] [G loss:2.928 acc:0.0625] [Q loss:0.005394]\n",
      "\n",
      "iter:78000\n",
      "[D loss:0.2971 acc:0.9375] [G loss:3.006 acc:0.03125] [Q loss:0.004443]\n",
      "\n",
      "iter:79000\n",
      "[D loss:0.407 acc:0.9375] [G loss:3.238 acc:0.0] [Q loss:0.004129]\n",
      "\n",
      "iter:80000\n",
      "[D loss:0.2616 acc:0.9531] [G loss:3.566 acc:0.0] [Q loss:0.002909]\n",
      "\n",
      "iter:81000\n",
      "[D loss:0.1453 acc:1.0] [G loss:2.992 acc:0.0] [Q loss:0.003983]\n",
      "\n",
      "iter:82000\n",
      "[D loss:0.3474 acc:0.9219] [G loss:4.138 acc:0.0] [Q loss:0.007086]\n",
      "\n",
      "iter:83000\n",
      "[D loss:0.4343 acc:0.9062] [G loss:3.003 acc:0.0625] [Q loss:0.003443]\n",
      "\n",
      "iter:84000\n",
      "[D loss:0.3867 acc:0.9062] [G loss:2.622 acc:0.03125] [Q loss:0.002043]\n",
      "\n",
      "iter:85000\n",
      "[D loss:0.2157 acc:0.9844] [G loss:3.053 acc:0.0] [Q loss:0.003373]\n",
      "\n",
      "iter:86000\n",
      "[D loss:0.09721 acc:1.0] [G loss:3.504 acc:0.0] [Q loss:0.002537]\n",
      "\n",
      "iter:87000\n",
      "[D loss:0.502 acc:0.9219] [G loss:2.789 acc:0.0] [Q loss:0.00248]\n",
      "\n",
      "iter:88000\n",
      "[D loss:0.2717 acc:0.9531] [G loss:3.145 acc:0.0] [Q loss:0.002175]\n",
      "\n",
      "iter:89000\n",
      "[D loss:0.5303 acc:0.9219] [G loss:3.055 acc:0.0] [Q loss:0.002256]\n",
      "\n",
      "iter:90000\n",
      "[D loss:0.3605 acc:0.9531] [G loss:2.763 acc:0.0] [Q loss:0.001904]\n",
      "\n",
      "iter:91000\n",
      "[D loss:0.271 acc:0.9688] [G loss:3.068 acc:0.03125] [Q loss:0.001949]\n",
      "\n",
      "iter:92000\n",
      "[D loss:0.5772 acc:0.9219] [G loss:3.788 acc:0.0] [Q loss:0.002694]\n",
      "\n",
      "iter:93000\n",
      "[D loss:0.4758 acc:0.9219] [G loss:3.077 acc:0.0] [Q loss:0.001768]\n",
      "\n",
      "iter:94000\n",
      "[D loss:0.5502 acc:0.9062] [G loss:2.883 acc:0.0] [Q loss:0.00248]\n",
      "\n",
      "iter:95000\n",
      "[D loss:0.5781 acc:0.8906] [G loss:2.872 acc:0.03125] [Q loss:0.002893]\n",
      "\n",
      "iter:96000\n",
      "[D loss:0.3833 acc:0.9219] [G loss:3.827 acc:0.0] [Q loss:0.001711]\n",
      "\n",
      "iter:97000\n",
      "[D loss:0.5665 acc:0.9219] [G loss:2.451 acc:0.03125] [Q loss:0.003047]\n",
      "\n",
      "iter:98000\n",
      "[D loss:0.3141 acc:0.9375] [G loss:3.525 acc:0.0] [Q loss:0.002735]\n",
      "\n",
      "iter:99000\n",
      "[D loss:0.2259 acc:0.9844] [G loss:2.915 acc:0.0] [Q loss:0.002069]\n",
      "\n",
      "iter:100000\n",
      "[D loss:0.387 acc:0.9375] [G loss:3.394 acc:0.03125] [Q loss:0.002203]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:101000\n",
      "[D loss:0.2995 acc:0.9688] [G loss:2.205 acc:0.03125] [Q loss:0.002272]\n",
      "\n",
      "iter:102000\n",
      "[D loss:0.2604 acc:0.9844] [G loss:3.385 acc:0.0] [Q loss:0.002726]\n",
      "\n",
      "iter:103000\n",
      "[D loss:0.2022 acc:0.9688] [G loss:3.559 acc:0.0] [Q loss:0.002849]\n",
      "\n",
      "iter:104000\n",
      "[D loss:0.3345 acc:0.9844] [G loss:2.842 acc:0.0] [Q loss:0.002703]\n",
      "\n",
      "iter:105000\n",
      "[D loss:0.4307 acc:0.8906] [G loss:3.643 acc:0.0] [Q loss:0.002921]\n",
      "\n",
      "iter:106000\n",
      "[D loss:0.1951 acc:0.9688] [G loss:3.469 acc:0.0] [Q loss:0.003464]\n",
      "\n",
      "iter:107000\n",
      "[D loss:0.5329 acc:0.9062] [G loss:2.235 acc:0.03125] [Q loss:0.0055]\n",
      "\n",
      "iter:108000\n",
      "[D loss:0.3877 acc:0.9375] [G loss:3.006 acc:0.03125] [Q loss:0.007164]\n",
      "\n",
      "iter:109000\n",
      "[D loss:0.3462 acc:0.9219] [G loss:3.21 acc:0.03125] [Q loss:0.003457]\n",
      "\n",
      "iter:110000\n",
      "[D loss:0.259 acc:0.9688] [G loss:3.05 acc:0.0] [Q loss:0.004779]\n",
      "\n",
      "iter:111000\n",
      "[D loss:0.579 acc:0.9219] [G loss:3.695 acc:0.0] [Q loss:0.003351]\n",
      "\n",
      "iter:112000\n",
      "[D loss:0.3494 acc:0.9062] [G loss:3.973 acc:0.03125] [Q loss:0.003769]\n",
      "\n",
      "iter:113000\n",
      "[D loss:0.2612 acc:0.9688] [G loss:2.72 acc:0.0] [Q loss:0.003171]\n",
      "\n",
      "iter:114000\n",
      "[D loss:0.1786 acc:0.9688] [G loss:3.438 acc:0.0] [Q loss:0.004243]\n",
      "\n",
      "iter:115000\n",
      "[D loss:0.1155 acc:0.9844] [G loss:4.257 acc:0.0] [Q loss:0.003684]\n",
      "\n",
      "iter:116000\n",
      "[D loss:0.1444 acc:0.9844] [G loss:4.144 acc:0.0] [Q loss:0.002698]\n",
      "\n",
      "iter:117000\n",
      "[D loss:0.1593 acc:0.9844] [G loss:3.596 acc:0.0] [Q loss:0.003972]\n",
      "\n",
      "iter:118000\n",
      "[D loss:0.1244 acc:1.0] [G loss:3.551 acc:0.0] [Q loss:0.002148]\n",
      "\n",
      "iter:119000\n",
      "[D loss:0.2507 acc:0.9531] [G loss:3.331 acc:0.03125] [Q loss:0.002866]\n",
      "\n",
      "iter:120000\n",
      "[D loss:0.04058 acc:1.0] [G loss:4.498 acc:0.0] [Q loss:0.005076]\n",
      "\n",
      "iter:121000\n",
      "[D loss:0.09383 acc:0.9844] [G loss:6.196 acc:0.0] [Q loss:0.01299]\n",
      "\n",
      "iter:122000\n",
      "[D loss:0.02497 acc:1.0] [G loss:4.989 acc:0.0] [Q loss:0.008417]\n",
      "\n",
      "iter:123000\n",
      "[D loss:0.01738 acc:1.0] [G loss:5.309 acc:0.0] [Q loss:0.009447]\n",
      "\n",
      "iter:124000\n",
      "[D loss:0.03112 acc:0.9844] [G loss:6.155 acc:0.0] [Q loss:0.01424]\n",
      "\n",
      "iter:125000\n",
      "[D loss:0.00535 acc:1.0] [G loss:6.019 acc:0.0] [Q loss:0.02189]\n",
      "\n",
      "iter:126000\n",
      "[D loss:0.002432 acc:1.0] [G loss:6.815 acc:0.0] [Q loss:0.01769]\n",
      "\n",
      "iter:127000\n",
      "[D loss:0.01471 acc:1.0] [G loss:6.197 acc:0.0] [Q loss:0.03793]\n",
      "\n",
      "iter:128000\n",
      "[D loss:0.02663 acc:1.0] [G loss:7.215 acc:0.0] [Q loss:0.01157]\n",
      "\n",
      "iter:129000\n",
      "[D loss:0.003287 acc:1.0] [G loss:5.764 acc:0.0] [Q loss:0.01263]\n",
      "\n",
      "iter:130000\n",
      "[D loss:0.01364 acc:1.0] [G loss:6.634 acc:0.0] [Q loss:0.06747]\n",
      "\n",
      "iter:131000\n",
      "[D loss:0.04257 acc:1.0] [G loss:4.445 acc:0.0] [Q loss:0.02216]\n",
      "\n",
      "iter:132000\n",
      "[D loss:0.02363 acc:1.0] [G loss:4.472 acc:0.0] [Q loss:0.02639]\n",
      "\n",
      "iter:133000\n",
      "[D loss:0.1261 acc:0.9844] [G loss:4.797 acc:0.0] [Q loss:0.01914]\n",
      "\n",
      "iter:134000\n",
      "[D loss:0.1492 acc:0.9844] [G loss:4.79 acc:0.0] [Q loss:0.01755]\n",
      "\n",
      "iter:135000\n",
      "[D loss:0.01153 acc:1.0] [G loss:6.253 acc:0.0] [Q loss:0.0489]\n",
      "\n",
      "iter:136000\n",
      "[D loss:0.01104 acc:1.0] [G loss:6.515 acc:0.0] [Q loss:0.03066]\n",
      "\n",
      "iter:137000\n",
      "[D loss:0.154 acc:0.9844] [G loss:5.568 acc:0.0] [Q loss:0.02404]\n",
      "\n",
      "iter:138000\n",
      "[D loss:0.1793 acc:0.9844] [G loss:4.117 acc:0.0] [Q loss:0.02109]\n",
      "\n",
      "iter:139000\n",
      "[D loss:0.01765 acc:1.0] [G loss:7.14 acc:0.0] [Q loss:0.02708]\n",
      "\n",
      "iter:140000\n",
      "[D loss:0.01181 acc:1.0] [G loss:4.778 acc:0.0] [Q loss:0.01914]\n",
      "\n",
      "iter:141000\n",
      "[D loss:0.006932 acc:1.0] [G loss:7.436 acc:0.0] [Q loss:0.01761]\n",
      "\n",
      "iter:142000\n",
      "[D loss:0.003766 acc:1.0] [G loss:7.088 acc:0.0] [Q loss:0.0265]\n",
      "\n",
      "iter:143000\n",
      "[D loss:0.00442 acc:1.0] [G loss:7.401 acc:0.0] [Q loss:0.0151]\n",
      "\n",
      "iter:144000\n",
      "[D loss:0.0234 acc:1.0] [G loss:6.672 acc:0.0] [Q loss:0.02603]\n",
      "\n",
      "iter:145000\n",
      "[D loss:0.002781 acc:1.0] [G loss:9.661 acc:0.0] [Q loss:0.07804]\n",
      "\n",
      "iter:146000\n",
      "[D loss:0.002187 acc:1.0] [G loss:12.33 acc:0.0] [Q loss:0.03644]\n",
      "\n",
      "iter:147000\n",
      "[D loss:0.009797 acc:1.0] [G loss:12.9 acc:0.0] [Q loss:0.03127]\n",
      "\n",
      "iter:148000\n",
      "[D loss:0.0007689 acc:1.0] [G loss:12.17 acc:0.0] [Q loss:0.02197]\n",
      "\n",
      "iter:149000\n",
      "[D loss:0.01003 acc:1.0] [G loss:10.15 acc:0.0] [Q loss:0.02737]\n",
      "\n",
      "iter:150000\n",
      "[D loss:0.005255 acc:1.0] [G loss:9.732 acc:0.0] [Q loss:0.03796]\n",
      "\n",
      "iter:151000\n",
      "[D loss:0.0004137 acc:1.0] [G loss:14.9 acc:0.0] [Q loss:0.02424]\n",
      "\n",
      "iter:152000\n",
      "[D loss:0.6718 acc:0.9531] [G loss:9.385 acc:0.0625] [Q loss:0.04278]\n",
      "\n",
      "iter:153000\n",
      "[D loss:0.02389 acc:1.0] [G loss:14.39 acc:0.0] [Q loss:0.02052]\n",
      "\n",
      "iter:154000\n",
      "[D loss:0.001447 acc:1.0] [G loss:12.12 acc:0.0] [Q loss:0.03152]\n",
      "\n",
      "iter:155000\n",
      "[D loss:0.000664 acc:1.0] [G loss:12.46 acc:0.0] [Q loss:0.0406]\n",
      "\n",
      "iter:156000\n",
      "[D loss:0.001457 acc:1.0] [G loss:11.39 acc:0.0] [Q loss:0.04516]\n",
      "\n",
      "iter:157000\n",
      "[D loss:0.001707 acc:1.0] [G loss:11.04 acc:0.0] [Q loss:0.03619]\n",
      "\n",
      "iter:158000\n",
      "[D loss:0.007616 acc:1.0] [G loss:13.9 acc:0.0] [Q loss:0.04931]\n",
      "\n",
      "iter:159000\n",
      "[D loss:0.0005009 acc:1.0] [G loss:12.06 acc:0.0] [Q loss:0.04961]\n",
      "\n",
      "iter:160000\n",
      "[D loss:0.001111 acc:1.0] [G loss:11.39 acc:0.0] [Q loss:0.03469]\n",
      "\n",
      "iter:161000\n",
      "[D loss:0.001082 acc:1.0] [G loss:13.9 acc:0.0] [Q loss:0.05887]\n",
      "\n",
      "iter:162000\n",
      "[D loss:0.001251 acc:1.0] [G loss:13.57 acc:0.0] [Q loss:0.0264]\n",
      "\n",
      "iter:163000\n",
      "[D loss:0.3112 acc:0.9844] [G loss:12.9 acc:0.0] [Q loss:0.03636]\n",
      "\n",
      "iter:164000\n",
      "[D loss:0.007185 acc:1.0] [G loss:10.48 acc:0.0] [Q loss:0.01816]\n",
      "\n",
      "iter:165000\n",
      "[D loss:0.01436 acc:1.0] [G loss:11.18 acc:0.0] [Q loss:0.01875]\n",
      "\n",
      "iter:166000\n",
      "[D loss:0.001642 acc:1.0] [G loss:13.64 acc:0.0] [Q loss:0.03549]\n",
      "\n",
      "iter:167000\n",
      "[D loss:0.006409 acc:1.0] [G loss:9.986 acc:0.0] [Q loss:0.03158]\n",
      "\n",
      "iter:168000\n",
      "[D loss:0.0004965 acc:1.0] [G loss:11.73 acc:0.0] [Q loss:0.02947]\n",
      "\n",
      "iter:169000\n",
      "[D loss:0.009929 acc:1.0] [G loss:9.247 acc:0.0] [Q loss:0.05255]\n",
      "\n",
      "iter:170000\n",
      "[D loss:0.001204 acc:1.0] [G loss:11.7 acc:0.0] [Q loss:0.04944]\n",
      "\n",
      "iter:171000\n",
      "[D loss:0.03467 acc:1.0] [G loss:9.44 acc:0.0] [Q loss:0.05011]\n",
      "\n",
      "iter:172000\n",
      "[D loss:0.0008372 acc:1.0] [G loss:10.24 acc:0.0] [Q loss:0.05044]\n",
      "\n",
      "iter:173000\n",
      "[D loss:0.001545 acc:1.0] [G loss:9.929 acc:0.0] [Q loss:0.03821]\n",
      "\n",
      "iter:174000\n",
      "[D loss:0.0008225 acc:1.0] [G loss:10.02 acc:0.0] [Q loss:0.03648]\n",
      "\n",
      "iter:175000\n",
      "[D loss:0.009872 acc:1.0] [G loss:9.104 acc:0.0] [Q loss:0.0503]\n",
      "\n",
      "iter:176000\n",
      "[D loss:0.1017 acc:0.9844] [G loss:7.108 acc:0.0] [Q loss:0.03342]\n",
      "\n",
      "iter:177000\n",
      "[D loss:0.006224 acc:1.0] [G loss:7.835 acc:0.0] [Q loss:0.01755]\n",
      "\n",
      "iter:178000\n",
      "[D loss:0.3513 acc:0.9844] [G loss:7.856 acc:0.0] [Q loss:0.03862]\n",
      "\n",
      "iter:179000\n",
      "[D loss:0.01829 acc:1.0] [G loss:9.375 acc:0.0] [Q loss:0.05485]\n",
      "\n",
      "iter:180000\n",
      "[D loss:0.08796 acc:0.9844] [G loss:9.062 acc:0.0] [Q loss:0.06454]\n",
      "\n",
      "iter:181000\n",
      "[D loss:0.1252 acc:0.9844] [G loss:9.2 acc:0.0] [Q loss:0.03911]\n",
      "\n",
      "iter:182000\n",
      "[D loss:0.01716 acc:1.0] [G loss:10.2 acc:0.0] [Q loss:0.01964]\n",
      "\n",
      "iter:183000\n",
      "[D loss:0.02073 acc:1.0] [G loss:6.543 acc:0.0] [Q loss:0.02085]\n",
      "\n",
      "iter:184000\n",
      "[D loss:0.07631 acc:0.9844] [G loss:10.19 acc:0.0] [Q loss:0.05678]\n",
      "\n",
      "iter:185000\n",
      "[D loss:0.03202 acc:1.0] [G loss:5.198 acc:0.0] [Q loss:0.03304]\n",
      "\n",
      "iter:186000\n",
      "[D loss:0.006295 acc:1.0] [G loss:6.48 acc:0.0] [Q loss:0.03101]\n",
      "\n",
      "iter:187000\n",
      "[D loss:0.06244 acc:0.9844] [G loss:6.269 acc:0.0] [Q loss:0.03142]\n",
      "\n",
      "iter:188000\n",
      "[D loss:0.02027 acc:1.0] [G loss:5.237 acc:0.0] [Q loss:0.05035]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "length = 8\n",
    "x_lins = np.linspace(-1.5, 1.5, length)\n",
    "y_lins = np.linspace(-1.5, 1.5, length)\n",
    "data_draw = np.vstack([np.hstack((x_lins[:,np.newaxis], np.ones([length, 1])*y_lins[length-1-i])) for i in range(length)])\n",
    "\n",
    "i=0\n",
    "for it in range(1000000):    \n",
    "    X_mb, _ =  mnist.train.next_batch(mb_size)\n",
    "#     Z_sample = sample_Z(mb_size, Z_dim)\n",
    "    Z_sample = np.zeros((mb_size, Z_dim))\n",
    "    c_sample_cat = sample_c_cat(mb_size, c_dim_cat)\n",
    "    c_sample_con = sample_c_con(mb_size, c_dim_con)\n",
    "        \n",
    "    _, D_l = sess.run([D_solver, D_loss], feed_dict={X:X_mb, Z:Z_sample, c_cat:c_sample_cat, c_con:c_sample_con})\n",
    "    _, G_l = sess.run([G_solver, G_loss], feed_dict={Z:Z_sample, c_cat:c_sample_cat, c_con:c_sample_con})\n",
    "    _, Q_l = sess.run([Q_solver, Q_loss], feed_dict={Z:Z_sample, c_cat:c_sample_cat, c_con:c_sample_con})\n",
    "    \n",
    "    if it%1000==0:\n",
    "        D_a = sess.run([D_accuracy], feed_dict={X:X_mb, Z:Z_sample, c_cat:c_sample_cat, c_con:c_sample_con})\n",
    "        G_a = sess.run([G_accuracy], feed_dict={Z:Z_sample, c_cat:c_sample_cat, c_con:c_sample_con})\n",
    "    \n",
    "        print('iter:{}'.format(it))\n",
    "        print('[D loss:{:.4} acc:{:.4}] [G loss:{:.4} acc:{:.4}] [Q loss:{:.4}]'.format(D_l, D_a[0], G_l, G_a[0], Q_l))\n",
    "        print()\n",
    "        \n",
    "    if it%20000==0:\n",
    "        for j in range(10):\n",
    "            n_sample = length*length\n",
    "\n",
    "            Z_sample = sample_Z(n_sample, Z_dim)\n",
    "            c_sample_cat = np.zeros([n_sample, c_dim_cat])\n",
    "            c_sample_cat[:, j] = 1\n",
    "            \n",
    "            c_sample_con = data_draw\n",
    "            \n",
    "            samples = sess.run(G_sample, feed_dict={Z:Z_sample, c_cat:c_sample_cat, c_con:c_sample_con})\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('out/{}.png'.format((str(i)+'_'+str(j)).zfill(3)), bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "        i+=1\n",
    "        \n",
    "    if it%1000==0:  \n",
    "        save_path = saver.save(sess, \"model/save_net.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
