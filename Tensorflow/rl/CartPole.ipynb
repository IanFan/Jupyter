{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part of code is the DQN brain, which is a brain of the agent.\n",
    "All decisions are made in here.\n",
    "Using Tensorflow to build the neural network.\n",
    "\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\n",
    "Using:\n",
    "Tensorflow: 1.0\n",
    "gym: 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep Q Network off-policy\n",
    "class DeepQNetwork:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.01,\n",
    "            reward_decay=0.9,\n",
    "            e_greedy=0.9,\n",
    "            replace_target_iter=300,\n",
    "            memory_size=500,\n",
    "            batch_size=32,\n",
    "            e_greedy_increment=None,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "\n",
    "        # total learning step\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # initialize zero memory [s, a, r, s_]\n",
    "        self.memory = np.zeros((self.memory_size, n_features * 2 + 2))\n",
    "\n",
    "        # consist of [target_net, evaluate_net]\n",
    "        self._build_net()\n",
    "        t_params = tf.get_collection('target_net_params')\n",
    "        e_params = tf.get_collection('eval_net_params')\n",
    "        self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        if output_graph:\n",
    "            # $ tensorboard --logdir=logs\n",
    "            # tf.train.SummaryWriter soon be deprecated, use following\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        self.cost_his = []\n",
    "\n",
    "    def _build_net(self):\n",
    "        # ------------------ build evaluate_net ------------------\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # input\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')  # for calculating loss\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            # c_names(collections_names) are the collections to store variables\n",
    "            c_names, n_l1, w_initializer, b_initializer = \\\n",
    "                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10, \\\n",
    "                tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers\n",
    "\n",
    "            # first layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n",
    "\n",
    "            # second layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_eval = tf.matmul(l1, w2) + b2\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        # ------------------ build target_net ------------------\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    # input\n",
    "        with tf.variable_scope('target_net'):\n",
    "            # c_names(collections_names) are the collections to store variables\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            # first layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)\n",
    "\n",
    "            # second layer. collections is used later when assign to target net\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_next = tf.matmul(l1, w2) + b2\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def action(self, observation):\n",
    "        observation = observation[np.newaxis, :]\n",
    "        actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "        action = np.argmax(actions_value)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # to have batch dimension when feed into tf placeholder\n",
    "        observation = observation[np.newaxis, :]\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # forward feed the observation and get q value for every actions\n",
    "            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # check to replace target parameters\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.replace_target_op)\n",
    "            print('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        # sample batch memory from all memory\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        q_next, q_eval = self.sess.run(\n",
    "            [self.q_next, self.q_eval],\n",
    "            feed_dict={\n",
    "                self.s_: batch_memory[:, -self.n_features:],  # fixed params\n",
    "                self.s: batch_memory[:, :self.n_features],  # newest params\n",
    "            })\n",
    "\n",
    "        # change q_target w.r.t q_eval's action\n",
    "        q_target = q_eval.copy()\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        reward = batch_memory[:, self.n_features + 1]\n",
    "\n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "\n",
    "        \"\"\"\n",
    "        For example in this batch I have 2 samples and 3 actions:\n",
    "        q_eval =\n",
    "        [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "\n",
    "        q_target = q_eval =\n",
    "        [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "\n",
    "        Then change q_target with the real q_target value w.r.t the q_eval's action.\n",
    "        For example in:\n",
    "            sample 0, I took action 0, and the max q_target value is -1;\n",
    "            sample 1, I took action 2, and the max q_target value is -2:\n",
    "        q_target =\n",
    "        [[-1, 2, 3],\n",
    "         [4, 5, -2]]\n",
    "\n",
    "        So the (q_target - q_eval) becomes:\n",
    "        [[(-1)-(1), 0, 0],\n",
    "         [0, 0, (-2)-(6)]]\n",
    "\n",
    "        We then backpropagate this error w.r.t the corresponding action to network,\n",
    "        leave other action as error=0 cause we didn't choose it.\n",
    "        \"\"\"\n",
    "\n",
    "        # train eval network\n",
    "        _, self.cost = self.sess.run([self._train_op, self.loss],\n",
    "                                     feed_dict={self.s: batch_memory[:, :self.n_features],\n",
    "                                                self.q_target: q_target})\n",
    "        self.cost_his.append(self.cost)\n",
    "\n",
    "        # increasing epsilon\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "    def save_mode(episode):\n",
    "        save_path = self.saver.save(sess, 'model/CartPole' + str(espisode) +'.ckpt')\n",
    "        print('Saved Model')\n",
    "        \n",
    "    def load_mode(espisode):\n",
    "        saver.restore(sess, 'model/CartPole' + str(espisode) +'.ckpt')\n",
    "        print('Load MOdel')\n",
    "\n",
    "    def plot_cost(self):\n",
    "        plt.plot(np.arange(len(self.cost_his)), self.cost_his)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('training steps')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0329 11:33:46.328931 4479096256 deprecation.py:506] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0329 11:33:46.333414 4479096256 deprecation.py:506] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py:187: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "episode:  0 ep_r:  5.99 epsilon:  0\n",
      "0\n",
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'env_wrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e04b14836982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0menv_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env_wrapper' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env = env.unwrapped\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "\n",
    "RL = DeepQNetwork(n_actions=env.action_space.n,\n",
    "                  n_features=env.observation_space.shape[0],\n",
    "                  learning_rate=0.01, e_greedy=0.9,\n",
    "                  replace_target_iter=100, memory_size=2000,\n",
    "                  e_greedy_increment=0.001,)\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = RL.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        \n",
    "        x, x_dot, theta, theta_dot = observation_\n",
    "        r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5\n",
    "        reward = r1 + r2\n",
    "        \n",
    "        RL.store_transition(observation, action, reward, observation_)\n",
    "\n",
    "        ep_r += reward\n",
    "        if total_steps > 1000:\n",
    "            RL.learn()\n",
    "\n",
    "        if done:\n",
    "            print('episode: ', i_episode,\n",
    "                  'ep_r: ', round(ep_r, 2),\n",
    "                  'epsilon: ', round(RL.epsilon, 2))\n",
    "            print(RL.epsilon)\n",
    "            print(RL.n_actions)\n",
    "            break\n",
    "\n",
    "        observation = observation_\n",
    "        total_steps += 1\n",
    "\n",
    "RL.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
