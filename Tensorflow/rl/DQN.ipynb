{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "if sys.version_info.major == 2:\n",
    "    import Tkinter as tk\n",
    "else:\n",
    "    import tkinter as tk\n",
    "\n",
    "UNIT = 40   # pixels\n",
    "MAZE_H = 4  # grid height\n",
    "MAZE_W = 4  # grid width\n",
    "\n",
    "\n",
    "class Maze(tk.Tk, object):\n",
    "    def __init__(self):\n",
    "        super(Maze, self).__init__()\n",
    "        self.action_space = ['u', 'd', 'l', 'r']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.n_features = 2\n",
    "        self.title('maze')\n",
    "        self.geometry('{0}x{1}'.format(MAZE_H * UNIT, MAZE_H * UNIT))\n",
    "        self._build_maze()\n",
    "\n",
    "    def _build_maze(self):\n",
    "        self.canvas = tk.Canvas(self, bg='white',\n",
    "                           height=MAZE_H * UNIT,\n",
    "                           width=MAZE_W * UNIT)\n",
    "\n",
    "        # create grids\n",
    "        for c in range(0, MAZE_W * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = c, 0, c, MAZE_H * UNIT\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "        for r in range(0, MAZE_H * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = 0, r, MAZE_H * UNIT, r\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # create origin\n",
    "        origin = np.array([20, 20])\n",
    "\n",
    "        # hell\n",
    "        hell1_center = origin + np.array([UNIT * 2, UNIT])\n",
    "        self.hell1 = self.canvas.create_rectangle(\n",
    "            hell1_center[0] - 15, hell1_center[1] - 15,\n",
    "            hell1_center[0] + 15, hell1_center[1] + 15,\n",
    "            fill='black')\n",
    "        # hell\n",
    "        # hell2_center = origin + np.array([UNIT, UNIT * 2])\n",
    "        # self.hell2 = self.canvas.create_rectangle(\n",
    "        #     hell2_center[0] - 15, hell2_center[1] - 15,\n",
    "        #     hell2_center[0] + 15, hell2_center[1] + 15,\n",
    "        #     fill='black')\n",
    "\n",
    "        # create oval\n",
    "        oval_center = origin + UNIT * 2\n",
    "        self.oval = self.canvas.create_oval(\n",
    "            oval_center[0] - 15, oval_center[1] - 15,\n",
    "            oval_center[0] + 15, oval_center[1] + 15,\n",
    "            fill='yellow')\n",
    "\n",
    "        # create red rect\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill='red')\n",
    "\n",
    "        # pack all\n",
    "        self.canvas.pack()\n",
    "\n",
    "    def reset(self):\n",
    "        self.update()\n",
    "        time.sleep(0.1)\n",
    "        self.canvas.delete(self.rect)\n",
    "        origin = np.array([20, 20])\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill='red')\n",
    "        # return observation\n",
    "        return (np.array(self.canvas.coords(self.rect)[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT)\n",
    "\n",
    "    def step(self, action):\n",
    "        s = self.canvas.coords(self.rect)\n",
    "        base_action = np.array([0, 0])\n",
    "        if action == 0:   # up\n",
    "            if s[1] > UNIT:\n",
    "                base_action[1] -= UNIT\n",
    "        elif action == 1:   # down\n",
    "            if s[1] < (MAZE_H - 1) * UNIT:\n",
    "                base_action[1] += UNIT\n",
    "        elif action == 2:   # right\n",
    "            if s[0] < (MAZE_W - 1) * UNIT:\n",
    "                base_action[0] += UNIT\n",
    "        elif action == 3:   # left\n",
    "            if s[0] > UNIT:\n",
    "                base_action[0] -= UNIT\n",
    "\n",
    "        self.canvas.move(self.rect, base_action[0], base_action[1])  # move agent\n",
    "\n",
    "        next_coords = self.canvas.coords(self.rect)  # next state\n",
    "\n",
    "        # reward function\n",
    "        if next_coords == self.canvas.coords(self.oval):\n",
    "            reward = 1\n",
    "            done = True\n",
    "        elif next_coords in [self.canvas.coords(self.hell1)]:\n",
    "            reward = -1\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "        s_ = (np.array(next_coords[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT)\n",
    "        return s_, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        # time.sleep(0.01)\n",
    "        self.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "class DeepQNetwork:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_acitons,\n",
    "            n_features,\n",
    "            learning_rate=0.01,\n",
    "            reward_decay=0.9,\n",
    "            e_greedy=0.9,\n",
    "            replace_target_iter=300,\n",
    "            memory_size=500,\n",
    "            batch_size=32,\n",
    "            e_greedy_increment=None,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_acitons\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replce_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        self.memory = np.zeros((self.memory_size, n_features*2 + 2))\n",
    "\n",
    "        self._build_net()\n",
    "        t_params = tf.get_collection('target_net_params')\n",
    "        e_params = tf.get_collection('eval_net_params')\n",
    "        self.replace_target_op = [tf.assign(t, e) for t,e in zip(t_params, e_params)]\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        if output_graph:\n",
    "            tf.summary.FileWriter('log/', self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.cost_his = []\n",
    "\n",
    "    def _build_net(self):\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            c_names = ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            n_l1 = 10\n",
    "            w_initializer = tf.random_normal_initializer(0., 0.3)\n",
    "            b_initializer = tf.constant_initializer(0.1)\n",
    "\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n",
    "\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_eval = tf.nn.relu(tf.matmul(l1, w2) + b2)\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "            # self.loss = tf.losses.mean_squared_error(self.q_target, self.q_eval)\n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.RMSPropOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')\n",
    "        with tf.variable_scope('target_net'):\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)\n",
    "\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_next = tf.nn.relu(tf.matmul(l1, w2) + b2)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "\n",
    "        # transition = np.hstack((s, [a], [r], s_))\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "\n",
    "        index = self.memory_counter %self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        observation = observation[np.newaxis, :]\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.learn_step_counter % self.replce_target_iter == 0:\n",
    "            self.sess.run(self.replace_target_op)\n",
    "            print('\\ntarget_params_replaces\\n')\n",
    "\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        q_next, q_eval = self.sess.run([self.q_next, self.q_eval],\n",
    "                                       feed_dict={\n",
    "                                            self.s_: batch_memory[:, -self.n_features:],\n",
    "                                            self.s: batch_memory[:, :self.n_features]\n",
    "                                       })\n",
    "        q_target = q_eval.copy()\n",
    "\n",
    "        batch_index = np.arange(0, self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        reward = batch_memory[:, self.n_features + 1]\n",
    "\n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "\n",
    "        _, self.cost = self.sess.run([self._train_op, self.loss],\n",
    "                                     feed_dict={\n",
    "                                         self.s: batch_memory[:, :self.n_features],\n",
    "                                         self.q_target: q_target\n",
    "                                     })\n",
    "\n",
    "        self.cost_his.append(self.cost)\n",
    "\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "    def plot_cost(self):\n",
    "        import  matplotlib.pylab as plt\n",
    "        plt.plot(np.arange(len(self.cost_his)), self.cost_his)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('train steps')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0324 16:21:30.820441 4419184064 deprecation.py:506] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0324 16:21:30.825243 4419184064 deprecation.py:506] From /Users/ianfan/anaconda3/envs/spinningup/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py:187: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_params_replaces\n",
      "\n",
      "\n",
      "target_params_replaces\n",
      "\n",
      "\n",
      "target_params_replaces\n",
      "\n",
      "\n",
      "target_params_replaces\n",
      "\n",
      "\n",
      "target_params_replaces\n",
      "\n",
      "\n",
      "target_params_replaces\n",
      "\n",
      "\n",
      "target_params_replaces\n",
      "\n",
      "game over\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_maze():\n",
    "    step = 0\n",
    "    for episode in range(300):\n",
    "        # initial observation\n",
    "        observation = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # fresh env\n",
    "            env.render()\n",
    "\n",
    "            # RL choose action based on observation\n",
    "            action = RL.choose_action(observation)\n",
    "\n",
    "            # RL take action and get next observation and reward\n",
    "            observation_, reward, done = env.step(action)\n",
    "\n",
    "            RL.store_transition(observation, action, reward, observation_)\n",
    "\n",
    "            if (step > 200) and (step % 5 == 0):\n",
    "                RL.learn()\n",
    "\n",
    "            # swap observation\n",
    "            observation = observation_\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                break\n",
    "            step += 1\n",
    "\n",
    "    # end of game\n",
    "    print('game over')\n",
    "    env.destroy()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # maze game\n",
    "    env = Maze()\n",
    "    RL = DeepQNetwork(env.n_actions, env.n_features,\n",
    "                      learning_rate=0.01,\n",
    "                      reward_decay=0.9,\n",
    "                      e_greedy=0.9,\n",
    "                      replace_target_iter=200,\n",
    "                      memory_size=2000,\n",
    "                      # output_graph=True\n",
    "                      )\n",
    "    env.after(100, run_maze)\n",
    "    env.mainloop()\n",
    "    RL.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
